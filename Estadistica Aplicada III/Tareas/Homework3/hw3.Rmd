---
title: "hw3"
author: "Pablo, Román, Sofia"
date: "25/2/2020"
output: html_document
---

```{r,echo = FALSE, include = FALSE,warning=FALSE}
library(dplyr)
library(ggplot2)
library(knitr)
library(factoextra)
library(devtools)
library(gridExtra)
```

```{r, echo = FALSE, include=FALSE}
set.seed(17032020) #created at date

number_ex <- 1:10 # 10 ejercicios
cat("\nEjericios Pablo: ")
(ex_pablo <- sample(x = number_ex,replace = F, size = 3))
number_ex <- number_ex[! number_ex %in% ex_pablo] #Removing questions

number_ex <- 1:10 # 10 ejercicios
cat("\nEjericios Roman: ")
(ex_roman <- sample(x = number_ex,replace = F, size = 3))
number_ex <- number_ex[! number_ex %in% ex_roman] #Removing questions

number_ex <- 1:10 # 10 ejercicios
cat("\nEjericios Sofia: ")
(ex_sofia <- sample(x = number_ex,replace = F, size = 3))
number_ex <- number_ex[! number_ex %in% ex_sofia] #Removing questions

```


## Ejercicio 1

```{r,echo = FALSE }
library('png')
```
! [ejercicio1](Tarea3.png)

## Ejercicio 2
Con los datos del archivo T8-4.DAT que corresponden a tasas de rendimiento de
cinco acciones listadas en NYSE:

##### a) Obtener la matriz muestral de covarianzas S y obtener las componentes principales.

Para eso scamos la matriz de covarianzas $S$ que es la siguiente.
```{r, echo=FALSE}
data_2 <- as.matrix(read.table("T84.DAT",header = FALSE))
(S2 <- cov(data_2))
```
Ahora para calcular las componentes principales encontraremos sus egienvecores.
```{r, echo=FALSE}
eig2 <- eigen(S2)
(y2 <- round(eig2$vectors,2)*-1) #lo multiplicamos por -1 simplemente por practicidad
```
 
De donde obtenemos los componentes principales.
$$\begin{array}{l}
Y_{1} = e_{1}^TX = 0.56X_{1} + 0.47X_{2}+ 0.55X_{3} + 0.29X_{4} + 0.28X_{5}\\
Y_{2} = e_{2}^TX = -0.74X_{1} + 0.09X_{2} + 0.65X_{3} + 0.11X_{4} - 0.07X_{5}\\
Y_{3} = e_{3}^TX = 0.13X_{1} + 0.47X_{2} +0.11X_{3} - 0.61X_{4} - 0.62X_{5}\\
Y_{4} = e_{4}^TX = -0.28X_{1} + 0.69X_{2} - 0.50X_{3} + 0.44X_{4} - 0.06X_{5}\\
Y_{5} = e_{5}^TX = 0.21X_{1} - 0.28X_{2} + 0.10X_{3} +  0.58X_{4} - 0.73X_{5}\\
\end{array}$$


##### b) Hacer la gráfica de codo correspondiente, y decidir cuáles son las componentes más significativas. Realizar una propuesta de interpretación para las dos primeras componentes.

Para realizar esto, veremos que tanta varianza aporta cada componente principal, mediante el siguiente razonamiento.

$$\frac{\lambda_{k}}{\lambda_{1} + ... + \lambda_{k}}$$
```{r,echo=FALSE}
aport_var <- function(l,lsum)
{
  l/lsum
}
lsum <- sum(eig2$values)

(var2 <-  aport_var(eig2$values,lsum))

plot(var2,type="o",ylab = "Porcentaje de varianza expresada", xlab = "Componente")
```

Podemos ver que la primer componente exlica un $60\%$ mientras que las componentes $Y_{2},Y_{3},Y_{4},$ explican casi un $10\%$ y la ultima componente unicamente $5\%$. La componente mas significativa es la primera.

Una interpretacion para la primer componente principal seria una especie de promedio ponderado pues todas las variables estan multiplicadas por un coeficiente que les da peso con respecto a que tanta varianza aportan. Mientras que la segunda componente principal podria ser una donde se contrasta la primera columna y la ultima contra el resto.

##### c) Construir los intervalos simultáneos bonferronizados de 90% para las varianzas λ1, λ2, λ3 de las primeras tres componentes Y1, Y2, Y3.

Para obtener los intervalos de confianza simultaneos de Bonferroni al $100(1-\alpha)\%$ de confianza para $\lambda_{i}$ se construiran de la siguiente manera.

$$\frac{\hat{\lambda_{i}}}{1 + z(\frac{\alpha}{2m})\sqrt{\frac{2}{n}}}\le \lambda_{i} \le \frac{\hat{\lambda_{i}}}{1 - z(\frac{\alpha}{2m})\sqrt{\frac{2}{n}}} $$
```{r,echo=FALSE}
n <- nrow(data_2) #número de datos
lambdas <- eig2$values #eigen-valores
alpha = .1
m = 3 #como estamos haciendo intervalos simultaneos para 3 lambdas
quantil = qnorm((alpha/2*m), mean = 0, sd = 1, lower.tail = FALSE)

(intervalo_1 <- c(lambdas[1]/(1 + quantil * sqrt(2/n)), lambdas[1]/(1 - quantil * sqrt(2/n))))
(intervalo_2 <- c(lambdas[2]/(1 + quantil * sqrt(2/n)), lambdas[2]/(1 - quantil * sqrt(2/n))))
(intervalo_3 <- c(lambdas[3]/(1 + quantil * sqrt(2/n)), lambdas[3]/(1 - quantil * sqrt(2/n))))
```


##### d)¿Se puede resumir la variabilidad de los datos en menos de 5 dimensiones? Explicar.

Si podriamos ya que con las primeras tres componentes principales $Y_{1},Y_{2},Y_{3}$ acumulamos un total del $80\%$ de viarianza acumulada, con lo cual podria representarse suficientemente bien el resto de los datos.


##### e) Replicar este ejercicio, pero tomando datos de 5 acciones de la Bolsa Mexicana de Valores, con los datos que puedan conseguir del año 2017.

Para eso Sacamos los rendimientos diarios de TV Azteca (AZTECACPO), Televisa (TELEVISACPO),Wal Mart Mexico (WALMEX), Chedrahui (CHDRAUIB) y Soriana (SORIANAB)
```{r, echo = FALSE}
data_2.2 <- read.csv("BMV2017.csv",header = TRUE)
head(data_2.2)
```
Sacamos su matriz de covarianza $S$ que es la siguiente.
```{r, echo=FALSE}
(S2.2 <- cov(data_2.2))
```
Sacamos sus componentes principales.


```{r, echo = FALSE}
eig2.2 <- eigen(S2.2)
(y2.2 <- round(eig2.2$vectors,2))
```
De donde obtenemos los siguientes.
$$\begin{array}{l}
Y_{1} = e_{1}^TX =  0.01X_{1} + 0.02X_{2} + 0.02X_{4} + X_{5}\\
Y_{2} = e_{2}^TX = -0.11X_{1} + 0.99X_{2} + 0.07X_{3} + 0.04X_{4} - 0.02X_{5}\\
Y_{3} = e_{3}^TX =  0.99X_{1} + 0.11X_{2} + 0.01X_{3} - 0.02X_{5}\\
Y_{4} = e_{4}^TX = -0.01X_{1} - 0.19X_{2} + 0.15X_{3} + 0.99X_{4} - 0.02X_{5}\\
Y_{5} = e_{5}^TX = - 0.06X_{2} + 0.99X_{3} + 0.16X_{4}\\
\end{array}$$

La grafica de codo es la siguiente.
```{r}
lsum2 <- sum(eig2.2$values)

(var2.2 <-  aport_var(eig2.2$values,lsum2))

plot(var2.2,type="o",ylab = "Porcentaje de varianza expresada", xlab = "Componente")
```

Podemos ver que la primer componente exlica un $50\%$ mientras que las componentes $Y_{2},Y_{3}$ explican casi un $25\%$ y la ultimas componentes $Y_{4},Y_{5}$ practicamente no explican nada. La componente pas significativa es la primera.

La interpretación es que son variables bastante ajenas aunque hubieramos pensado que habria alguna relación entre las acciones de las televisoras y las acciones de los supermercados.
La primer componente $Y_{1}$ le otorga todo el peso a la variable *Soriana* mientras que la segunda $Y_{2}$ muestra una relacion inversa entre las televisoras dandole mayo peso a *Televisa*. La tercera $Y_3$ practicamente es la inversa de $Y_{2}$ da todo el peso a *TV Azteca* y un peso igual de pequeño a *Televisa* de esta podriamos deducir si no lo supieramos que estan en competencia y son bienes sustitutos. Para $Y_4$ le da practicamente todo el peso a *Soriana*, y pesos casi cero al resto.$Y_5$ es la unica que agrupa de manera positiva a *Walmart* con *Soriana* dandole casi todo el peso a *Walmart*, como esta casi no aporta varinza al modelo podriamos interpretar que es una variable algo ajena en los datos. Lo cual hace sentido.

Acontinuación mostraremos los intervalos simultaneos de bonferroni para $\lambda_i$ con $i = 1,2,3$.
```{r,echo=FALSE}
n <- nrow(data_2.2) #número de datos
lambdas.2 <- eig2.2$values #eigen-valores
alpha = .1
m = 3 #como estamos haciendo intervalos simultaneos para 3 lambdas
quantil = qnorm((alpha/2*m), mean = 0, sd = 1, lower.tail = FALSE)

(intervalo_1 <- c(lambdas.2[1]/(1 + quantil * sqrt(2/n)), lambdas[1]/(1 - quantil * sqrt(2/n))))
(intervalo_2 <- c(lambdas.2[2]/(1 + quantil * sqrt(2/n)), lambdas[2]/(1 - quantil * sqrt(2/n))))
(intervalo_3 <- c(lambdas.2[3]/(1 + quantil * sqrt(2/n)), lambdas[3]/(1 - quantil * sqrt(2/n))))
```

Podemos concluir este ejercicio diciendo que no bastaria con las primeras dos componentes principales para explicar la variabilidad de los datos, sino con las primeras tres componentes principales $Y_{1},Y_{2},Y_{3}$ acumulamos un total de casi $99\%$ de viarianza acumulada.



## Ejercicio 3
##### 3a. Comparar estimaciones
```{r, echo = FALSE }
df <- read.table('T85.DAT')

#change of the last column
df_alter <- df
df_alter[,5] <- df[,5] * 10 # part of excerise 

#3.a. Compare S_original vs S_altered
CP_o <- princomp(df)
CP_a <- princomp(df_alter)
rel_error <- norm(CP_o$scores - CP_a$scores, type = 'I')/norm(CP_o$scores, type = 'I')

lambdas_rel_err <- (CP_o$sdev - CP_a$sdev)/CP_o$sdev

```

Veamos:

- El _error relativo para los scores_ de las CP's, calculado por la $|| . ||_{\infty}$ es: `r rel_error`
- El vector con los errores relativos para $\lambda$'s estimadas es 
```{r, echo=FALSE}
kable(lambdas_rel_err, col.names = 'lambda')
```

Se puede concluir que el cambio de escala SÍ afecta la estimación de los componentes principales, y por mucho.

##### 3b. Interpretación
```{r cpa analisis, echo=FALSE}
#plots
#scree
g1_3b <- fviz_screeplot(CP_o)
g2_3b <- fviz_screeplot(CP_a)

grid.arrange(g1_3b, g2_3b, nrow = 1)

#biplot
g3_3b <- fviz_pca_biplot(CP_o)
g4_3b <- fviz_pca_biplot(CP_a)


grid.arrange(g3_3b,g4_3b, nrow = 1)

#interpretation
loadings(CP_o)
loadings(CP_a)
```

Para el caso de los datos originales, $Y_{1}$ es una ponderación de las 5 variables, dándole mayor peso a la primer variable. La segunda CP es una distinción entre la segunda variable y quinta variable contra la cuarta.

En el caso de los datos modificados, $Y_{1}^{'}$ se inclina _totalmente_ hacia la última variable. Mientras que la segunda componente es un ponderaje de las primeras cuatro variables originales, se observa con mayor claridad en el _biplot_.

##### 3c. Describir efectos
Vemos que el cambio de escala distorciona por completo los componentes principales y por ende su interpretación. Es recomendable en este caso usar $R$, o lo que es lo mismo, _estandarizar_ los datos (studentizarlos).

## Ejercicio 4
Cargamos los datos y elegimos los que queremos analizar.
```{r}
DATA<-read.table("T110.DAT", header=F)
X<-DATA[,-c(1,2)]
```

Estandarizamos los datos porque sus unidades son distintas
```{r}
X_std <- scale(X)
```

Obtenemos la matriz de covarianzas

```{r}
cov<-cov(X_std)
```

Encontramos sus eigenvalores y eigenvectores. Un vector propio es una dirección y un valor propio es un número que indica cuánta varianza hay en los datos en esa dirección.

```{r}
ei<-eigen(cov)
```

El valor propio más grande es el primer componente principal; multiplicamos los valores estandarizados al primer vector propio. Aplicamos lo mismo para todos los vectores propios.

```{r}
comp <- matrix(data=NA, nrow=76, ncol=7)
for(j in 1:7){
    comp[,j] <- X_std %*% ei$vectors[,j]
   }
```

Mostemos que obtenemos lo mismo haciendolo con la matriz de correlaciones:
```{r}
cor<-cor(X_std)
ei_r<-eigen(cor)
comp_r <- matrix(data=NA, nrow=76, ncol=7)
for(i in 1:7){
    comp_r[,i] <- X_std %*% ei_r$vectors[,i]
   }
```

Calculamos la proporción de varianza total que explica cada componente y la acumulada
```{r}
eigv<-ei$values
rbind(
  SD = sqrt(eigv),
  Proportion = eigv/sum(eigv),
  Cumulative = cumsum(eigv)/sum(eigv))
```

##### a)Determinar el número apropiado de componentes que resumen adecuadamente la variabilidad de los datos originales.

De acuerdo al cálculo anterior, las primeras 4 componentes explican el 95% de la variabilidad total.

##### b)Interpretación de las componentes principales.
```{r}
(ei$vector)
```

Para la primera componente principal el signo solo cambia para la variable de grasa trasera, entonces podría interpretarse como un contraste entre la configuración "valiosa" y "no valiosa" en términos de carne del cuerpo.

La segunda tiene que la grasa trasera tiene el mayor peso, entonces se podría decir que la componente pondera las partes más pesadas.

La tercera contrasta el peso por venta dado cuanto del cuerpo estaba libre de grasa.

la cuarta contrasta lo que tiene el cuerpo por lo que se vende.

La quinta podría tener una interpretación parecida a la tercera.

La sexta contrapone las partes que no se venden del cuerpo.

La última resalta la medición al año y al momento de venta.

##### c)¿Será posible desarrollar un índice ‘Tamaño de cuerpo’ o ‘configuración de cuerpo’ basado en las 7 variables consideradas? Expliquen

Fijandos en la primera componente, me parece que se podría obtener un buen indice. En donde la grasa trasera indicaría oposición con el resto de las variables.

##### d) Hacer una gráfica de las dos primeras componentes. ¿Hay outliers? Si los hay, hacer una sustitución de la matriz de covarianzas con una matriz de covarianzas estimada de manera robusta.

```{r}
plot(comp[,1:2], pch = 16, cex = 0.1)
text(comp, labels = 1:88, cex = 0.7) 
abline(h=0);abline(v=0)
```
Se aprecia que hay, por lo menos, dos outliers.
Sustituimos por una matriz de covarianzas estimada de manera robuzta. 
```{r}
cov_rob<-cov(X_std,method="spearman")
ei_rob<-eigen(cov_rob)
comp_rob <- matrix(data=NA, nrow=76, ncol=7)
for(i in 1:7){
    comp_rob[,i] <- X_std %*% ei_rob$vectors[,i]
}
plot(comp_rob[,1:2], pch = 16, cex = 0.1)
text(comp_rob, labels = 1:88, cex = 0.7) 
abline(h=0);abline(v=0)
```

###### e) Evalúen si los datos originales son normales. Si no lo son, buscar las transformaciones que los acerquen a normalidad. Repetir el análisis con los datos transformados y probar la significancia de la varianza de las componentes principales con el resultado de Anderson

```{r}
lapply(X, shapiro.test)
```

Rechazamos la hipótesis de que sea normal para $V_4$, $V_6$, $V_7$. Entonces transformamos con la función Box Cox esos 3 vectores.

```{r}
library(car)
library(forecast)
X_norm<-X
for (i in c(2,4,5)){
  X_norm[,i]<-bcPower(X[,i],lambda=BoxCox.lambda(X[,i], method = "loglik", lower = 0, upper = 1) )
}
```

```{r}
lapply(X_norm, shapiro.test)
```

Los vectores $V_4$, $V_6$, $V_7$ siguen sin cumplir con la hipótesis de normalidad, pero se acercan a esto más que antes.

Calculamos todo de nuevo:

```{r, echo = FALSE}
X_norm<-as.matrix(X_norm)
co<-cov(X_norm)
e<-eigen(co)
com<- matrix(data=NA, nrow=76, ncol=7)
for(i in 1:7){
    com[,i] <- X_norm %*% e$vectors[,i]
}
plot(com[,1:2], pch = 16, cex = 0.1)
text(com, labels = 1:88, cex = 0.7) 
abline(h=0);abline(v=0)
```

Notemos que los valores propios son:

```{r, echo = FALSE}
e$values
```

La diferencia entre los dos últimos es muy pequeña:
```{r, echo = FALSE}
e$values[6]-e$values[7]
```

la diferencia entre el último y el antepenúltimo también es pequeña:
```{r, echo = FALSE}
e$values[5]-e$values[6]
```

Esto se cumple para los últimos tres eigenvalores:
```{r, echo = FALSE}
(dif1<-e$values[6]-e$values[7])
(dif2<-e$values[5]-e$values[6])
(dif3<-e$values[4]-e$values[5])
```

Se prueba, entonces, la significancia de los componentes grandes.Probando así el resultado de Anderson.





## Ejercicio 5
Consideren la matriz de correlaciones siguiente. Los datos originales corresponden a las mediciones de 8 variables de química sanguínea de 72 pacientes en un estudio clínico. (Jolliffe, 2002). La matriz de correlaciones de las variables *rblood*, *plate*, *wblood*, *neut*, *lymph*, *bilir*, *sodium* y *potass*, en ese orden, es la siguiente:

```{r, echo=F}
(data_5 <- matrix(c(1.000, 0.290, 0.202, -0.055, -0.105, -0.252, -0.229, 0.058, 0.290, 1.000, 0.415, 0.285, -0.376, -0.349, -0.164, -0.129, 0.202, 0.415, 1.000, 0.419, -0.521, -0.441, -0.145, -0.076, -0.055, 0.285, 0.419, 1.000, -0.877, -0.076, 0.023, -0.131, -0.105, -0.376, -0.521 ,-0.877, 1.000, 0.206, 0.034, 0.151, -0.252, -0.349, -0.441, -0.076, 0.206, 1.000, 0.192, 0.077,-0.229, -0.164, -0.145, 0.023,0.034, 0.192, 1.000, 0.423, 0.058, -0.129, -0.076, -0.131, 0.151, 0.077, 0.423, 1.000), ncol = 8))
```
y las desviaciones estándar, que tienen considerales diferencias, son:
```{r,, echo=F}
sd5 <- c(0.371, 41.253, 1.935, 0.077, 0.071, 4.037, 2.732, 0.297)
names(sd5) <- c("rblood", "plate", "wblood", "neut", "lymph", "bilir", "sodium","potass")
sd5
```

##### a)Aplicar componentes principales a la matriz de covarianzas y a la matriz de correlaciones. Explicar las diferencias.

Primero vermos los resultados para la matriz de covarianzas y esto lo haremos para simplificar el computo con la funcion de **R**, *princomp*.
```{r, echo=FALSE}
z5.1 <- princomp(data_5)
summary(z5.1, loadings=T)
```
Podemos ver que los componentes principales obtenidos de la matriz de covarianza, son los siguientes.
$$\begin{array}{l}
Y_{1} =  0.184X_{1} + 0.372X_{2}+ 0.452X_{3} + 0.426X_{4} - 0.491X_{5} - 0.328X_{6} - 0.199X_{7} - 0.196X_{8}\\
Y_{2} =  0.5X_{1} + 0.195X_{2} - 0.498X_{4} + 0.38X_{5} - 0.329X_{6} - 0.466X_{7} - 0.105X_{8}\\
Y_{3} =  0.153X_{1} + 0.180X_{3} - 0.467X_{6} + 0.480X_{7} + 0.696X_{8}\\
Y_{4} =  0.715X_{1} - 0.228X_{2} - 0.269X_{3} + 0.214X_{4} - 0.283X_{5} + 0.371X_{6} - 0.136X_{7} + 0.287X_{8}\\
Y_{5} =  0.756X_{2} - 0.535X_{3} + 0.344X_{7} \\
Y_{6} =  0.357X_{1} - 0.349X_{2} - 0.213X_{6} + 0.573X_{7} - 0.612X_{8}\\
Y_{7} =  0.142X_{1} + 0.231X_{2} + 0.613X_{3} -  0.352X_{4} + 0.606X_{6} + 0.244X_{7}\\
Y_{8} =  0.157X_{1}  + 0.168X_{3} +  0.625X_{4} + 0.730X_{5}\\
\end{array}$$


```{r, echo = FALSE}
z5.2 <- princomp(data_5,cor = T)
summary(z5.2, loadings=T)
```

Podemos ver que los componentes principales obtenidos de la matriz de correlación, son los siguientes.
$$\begin{array}{l}
Y_{1} =  0.266X_{1} + 0.421X_{2}+ 0.428X_{3} + 0.344X_{4} - 0.389X_{5} - 0.369X_{6} - 0.292X_{7} - 0.279X_{8}\\
Y_{2} =  0.521X_{1} - 0.124X_{3} - 0.516X_{4} + 0.414X_{5} - 0.17X_{6} - 0.482X_{7} - 0.101X_{8}\\
Y_{3} =  0.293X_{1} + 0.156X_{3} - 0.449X_{6} + 0.373X_{7} + 0.733X_{8}\\
Y_{4} =  0.615X_{1} - 0.252X_{2} - 203X_{3} + 0.312X_{4} - 0.322X_{5} + 0.493X_{6} - 0.144X_{7} + 0.229X_{8}\\
Y_{5} =  0.287X_{1} + 0.534X_{2} - 0.499X_{3} + 0.566X_{7} -0.322X_{8} \\
Y_{6} =  0.296X_{1} - 0.643X_{2} - 0.238X_{3} + -0.243X_{6} + 0.406X_{7} - 0.467X_{8}\\
Y_{7} =  0.104X_{1} + 0.215X_{2} + 0.673X_{3} -  0.363X_{4} + 0.567X_{6} + 0.186X_{7}\\
Y_{8} =  0.117X_{1} + 0.155X_{3} +  0.613X_{4} + 0.755X_{5}\\
\end{array}$$

Podemos al ver que al comparar entre ambos componentes principales, $Y_{1}$ no se ve alterado en signos ni tampoco cambio significativo en las magnitudes. Mientras que en $Y_{2}$ el unico cambio significativo es que se cambia *plate* por *wblood*. $Y_{5}$ con ma matriz de covarianza toma en cuenta las variables *rblod*,*sodium* y *potass*. En la componente $Y_{6}$ se añade la variable *wblood*. $Y_{3}$,$Y_{4}$,$Y_{7}$ $Y_{8}$ no presenta cambios significativos.
A pesar de estos camibios podrian parecer sutiles, podemos ver que las variables que mas se añaden a las componentes principales son *wblood* y *rblood*.

Para el caso de la matriz de covarianzas basta con las primeras dos componentes principales para explicar un $80\%$ de la varianza. Mientras que no es asi con las componentes principales obtenidas de la matriz de correlación.


##### b)Basado en la observación anterior ¿sobre qué debería hacerse el análisis?

Como las unidades de globulos rojos y globulos blancos se miden (por lo genenral) en microlitros de sangre, mientras que variables como potacio se mide en milimoles por litro, sodio en miliequivalentes por litro, las plaquetas en  microlitros, bilirrubina en miligramos, etc. 
Como no tenemos informacion concreta sobre las mediciones de estos componentes **la mejor opción es usar los componentes pricnipales de la matriz de correlación**.


## Ejercicio 6
```{r}
matriz<-cbind(c(1,0.402,0.396,0.301,0.305,0.339,0.340),c(0.402,1,0.618,0.150,0.135,0.206,0.183),c(0.396,0.618,1,0.321,0.289,0.363,0.345),c(0.301,0.15,0.321,1,0.846,0.759,0.661),c(0.305,0.135,0.289,0.846,1,0.797,0.8),c(0.339,0.206,0.363,0.759,0.797,1,0.736),c(0.340,0.183,0.345,0.661,0.8,0.736,1))
eigen<-eigen(matriz)
loadings<-eigen$vectors
```

```{r}
z<-princomp(covmat=matriz)
z$loadings
```
